from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
)
import pandas as pd
from sklearn.model_selection import train_test_split

# === 1Ô∏è‚É£ Charger ton CSV local ===
df = pd.read_csv("data/sentiment_dataset.csv")

# üßπ Nettoyage du dataset avant split
df = df.dropna(subset=["label", "text"])  # enl√®ve les lignes sans texte ou label
df["label"] = df["label"].astype(str).str.strip()  # supprime espaces inutiles
df = df[df["label"] != ""]  # supprime lignes avec label vide

# === 2Ô∏è‚É£ Diviser en train/test ===
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df.to_csv("data/train.csv", index=False)
test_df.to_csv("data/test.csv", index=False)

dataset = load_dataset(
    "csv", data_files={"train": "data/train.csv", "test": "data/test.csv"}
)

# === 3Ô∏è‚É£ Encoder les labels ===

df["label"] = df["label"].fillna("").astype(str)
labels = sorted(df["label"].unique().tolist())
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for label, i in label2id.items()}
print("Label mapping:", label2id)

# === 4Ô∏è‚É£ Tokenizer + mod√®le de base ===
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True,
)


# === NEW: transformer la colonne 'label' en 'labels' num√©riques
def label_to_id(example):
    return {"labels": label2id[example["label"]]}


dataset = dataset.map(label_to_id)

# Supprimer la colonne texte 'label' (on garde 'labels' num√©rique)
dataset = dataset.remove_columns(["label"])


# === 5Ô∏è‚É£ Pr√©parer les tokens ===
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True)


tokenized_datasets = dataset.map(preprocess_function, batched=True)

# === 6Ô∏è‚É£ Arguments d'entra√Ænement ===
training_args = TrainingArguments(
    output_dir="model/finetuned-sentiment",
    eval_strategy="epoch",  # <= was evaluation_strategy
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=10,
    report_to="none",  # √©vite les warnings de loggers
    seed=42,
)


# === 7Ô∏è‚É£ Trainer ===
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
)

# === 8Ô∏è‚É£ Lancer l'entra√Ænement ===
trainer.train()

# === 9Ô∏è‚É£ Sauvegarder le mod√®le fine-tun√© ===
trainer.save_model("model/finetuned-sentiment")
tokenizer.save_pretrained("model/finetuned-sentiment")

print("‚úÖ Fine-tuning termin√© et mod√®le sauvegard√© dans model/finetuned-sentiment/")
